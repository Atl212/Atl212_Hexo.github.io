<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《白话机器学习的数学》正则化实现代码</title>
    <url>/Atl212_Hexo.github.io/2021/10/18/%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E6%AD%A3%E5%88%99%E5%8C%96%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p>实现代码：</p>
<span id="more"></span> 

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"> </span><br><span class="line"># 真正的函数</span><br><span class="line">def g(x):</span><br><span class="line">    return 0.1 * (x ** 3 + x ** 2 + x)</span><br><span class="line"> </span><br><span class="line"># 随意准备一些向真正的函数加入了一点噪声的训练数据</span><br><span class="line">train_x = np.linspace (-2,2,8)</span><br><span class="line">train_y =g(train_x)+ np.random.randn(train_x.size) * 0.05</span><br><span class="line"> </span><br><span class="line"># 绘图确认</span><br><span class="line">x = np.linspace(-2,2,100)</span><br><span class="line">plt.plot(train_x, train_y, &#x27;o&#x27;)</span><br><span class="line">plt.plot(x, g(x), linestyle = &#x27;dashed&#x27;)</span><br><span class="line">plt.ylim(-1, 2)</span><br><span class="line">plt.show()</span><br><span class="line"> </span><br><span class="line">##########</span><br><span class="line"> </span><br><span class="line"># 标准化</span><br><span class="line">mu = train_x.mean ()</span><br><span class="line">sigma = train_x.std()</span><br><span class="line">def standardize (x):</span><br><span class="line">    return (x - mu)/ sigma</span><br><span class="line"> </span><br><span class="line">train_z = standardize(train_x)</span><br><span class="line"> </span><br><span class="line"># 创建训练数据的矩阵</span><br><span class="line">def to_matrix(x):</span><br><span class="line">    return np.vstack([ </span><br><span class="line">        np.ones(x.size),</span><br><span class="line">        x,    </span><br><span class="line">        x ** 2,</span><br><span class="line">        x ** 3,</span><br><span class="line">        x ** 4,</span><br><span class="line">        x ** 5,</span><br><span class="line">        x ** 6,</span><br><span class="line">        x ** 7,                                           </span><br><span class="line">        x ** 8,</span><br><span class="line">        x ** 9, </span><br><span class="line">        x ** 10,</span><br><span class="line">        ]).T </span><br><span class="line"> </span><br><span class="line">X = to_matrix(train_z)</span><br><span class="line"> </span><br><span class="line">#参数初始化</span><br><span class="line">theta= np.random.randn(X.shape[1])</span><br><span class="line"> </span><br><span class="line"># 预测函数</span><br><span class="line">def f (x):</span><br><span class="line">    return np.dot (x,theta)</span><br><span class="line"> </span><br><span class="line">##########</span><br><span class="line"> </span><br><span class="line"># 目标函数</span><br><span class="line">def E(x, y):</span><br><span class="line">     return 0.5 * np.sum((y - f(x)) ** 2)</span><br><span class="line"> </span><br><span class="line"># 学习率</span><br><span class="line">ETA = 1e-4</span><br><span class="line"> </span><br><span class="line"># 误差</span><br><span class="line">diff = 1</span><br><span class="line"># 重复学习</span><br><span class="line">error = E(X,train_y)</span><br><span class="line">while diff &gt; 1e-6:</span><br><span class="line">    theta= theta - ETA * np.dot(f(X) - train_y,X)</span><br><span class="line">    current_error = E(X,train_y)</span><br><span class="line">    diff = error - current_error</span><br><span class="line">    error = current_error</span><br><span class="line">    </span><br><span class="line"># 对结果绘图</span><br><span class="line">z = standardize(x)</span><br><span class="line">plt.plot(train_z, train_y, &#x27;o&#x27;)</span><br><span class="line">plt.plot(z, f(to_matrix(z)))</span><br><span class="line">plt.show()</span><br><span class="line"> </span><br><span class="line">##########</span><br><span class="line"> </span><br><span class="line"># 保存未正则化的参数，然后再次参数初始化</span><br><span class="line">theta1 = theta</span><br><span class="line">theta = np.random.randn(X.shape[1])</span><br><span class="line"> </span><br><span class="line"># 正则化常量</span><br><span class="line">LAMBDA = 1</span><br><span class="line"> </span><br><span class="line">#误差</span><br><span class="line">diff = 1</span><br><span class="line"> </span><br><span class="line">#重复学习（包含正则化项）</span><br><span class="line">error = E(X, train_y)</span><br><span class="line">while diff &gt; 1e-6:</span><br><span class="line">    # 正则化项。偏置项不适用正则化，所以为 0</span><br><span class="line">    reg_term = LAMBDA * np.hstack([0, theta[1:]])</span><br><span class="line">    # 应用正则化项，更新参数</span><br><span class="line">    theta = theta - ETA *(np.dot(f(X) - train_y,X)+ reg_term) </span><br><span class="line">    current_error = E(X, train_y)</span><br><span class="line">    diff = error - current_error</span><br><span class="line">    error = current_error</span><br><span class="line"> </span><br><span class="line"># 对结果绘图</span><br><span class="line">plt.plot(train_z,train_y,&#x27;o&#x27;)</span><br><span class="line">plt.plot(z,f(to_matrix(z)))</span><br><span class="line">plt.show()</span><br><span class="line"> </span><br><span class="line">##########</span><br><span class="line"> </span><br><span class="line"># 保存应用了正则化的参数</span><br><span class="line">theta2 = theta</span><br><span class="line"> </span><br><span class="line">plt.plot(train_z, train_y, &#x27;o&#x27;)</span><br><span class="line"> </span><br><span class="line"># 画出未应用正则化的结果</span><br><span class="line">theta = theta1</span><br><span class="line">plt.plot(z, f(to_matrix(z)), linestyle = &#x27;dashed&#x27;)</span><br><span class="line"># 画出应用了正则化的结果</span><br><span class="line">theta = theta2</span><br><span class="line">plt.plot(z, f(to_matrix(z)))</span><br><span class="line"> </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
        <tag>正则化</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>某类幂函数不定积分的步骤</title>
    <url>/Atl212_Hexo.github.io/2021/01/19/%E6%9F%90%E7%B1%BB%E5%B9%82%E5%87%BD%E6%95%B0%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86%E7%9A%84%E6%AD%A5%E9%AA%A4/</url>
    <content><![CDATA[<p>$$<br>\int x^{\alpha}dx &#x3D; \int \frac{x^{a+1}}{\alpha+1}+c(\alpha\neq-1)<br>$$</p>
<p>我们知道幂函数的不定积分公式一般都可以表示为这个式子（除了指数等于-1），但是如果每次都按照式子的步骤来写就会感到麻烦。</p>
<p>当a是整数时候还好，如果a是”分数”或者是”负数”或者是”负数的分数”时候，因为分母的系数是在下方所以要除了算出来外，最后还要倒过来。</p>
<p>特别是负数的分数如 -（1 &#x2F; 2）时，-（1 &#x2F; 2）+ 1 &#x3D; （1 &#x2F; 2）倒过来系数为2，本人在因为在运算中跳步错过几次，吃了亏。</p>
<p>那么有没有更简洁的方法，保证我们运算简单点并且保证准确性。</p>
<span id="more"></span> 

<p>通过观察式子我们可以发现，幂函数的指数和系数是一样的，都是a + 1，不同在于系数要倒过来。所以运算时我们可以这样。</p>
<p>$$<br>\int x^{\alpha}dx \rightarrow x^{\alpha+1} \rightarrow \frac{1}{\alpha+1}x^{a+1}+c<br>$$</p>
<p>先写出指数加一得到的数，然后把得到的指数倒过来就变成了前面的系数。举例子：</p>
<p>$$<br>\int x^{-\frac{1}{2}}dx \rightarrow x^{\frac{1}{2}} \rightarrow 2x^{\frac{1}{2}}+c<br>$$</p>
<p>$$<br>\int x^{-\frac{4}{3}}dx \rightarrow x^{-\frac{1}{3}} \rightarrow -3x^{-\frac{1}{3}}+c<br>$$</p>
<p>写出来后不要忘记加c即可。</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>高等数学</tag>
        <tag>不定积分</tag>
      </tags>
  </entry>
</search>
